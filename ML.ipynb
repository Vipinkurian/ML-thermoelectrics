{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyx36iNCDtN_"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matminer\n",
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "id": "DXNxxtqJJlV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import tensorflow as tf\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from matminer.featurizers.base import MultipleFeaturizer\n",
        "from matminer.featurizers import composition as cf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(44)\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "from pymatgen.core import Composition\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import initializers, regularizers\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "A9lGr3lfuHlf",
        "outputId": "43ce53b7-230a-47d3-c678-eb4e03823c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"dataset.xlsx\"\n",
        "data = \"dataset.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "property_interest = 'thermal_conductivity(W/mK)'\n",
        "print(df.head())\n",
        "\n",
        "num_rows = df.shape[0]\n",
        "print(f\"The number of rows in the DataFrame is: {num_rows}\")"
      ],
      "metadata": {
        "id": "MvXIaoMkxhVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "file_path = \"dataset.xlsx\"\n",
        "try:\n",
        "    data = pd.read_excel(file_path)\n",
        "    print(\"Data loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    data = None\n",
        "if isinstance(data, pd.DataFrame):\n",
        "    print(data.columns)\n",
        "    print(data.head())\n",
        "else:\n",
        "    print(\"Data is not a valid DataFrame.\")\n"
      ],
      "metadata": {
        "id": "pPF6GPIId-SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_compostion(c):\n",
        "    try:\n",
        "        return Composition(c)\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "uKkGt7e6xH4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymatgen.core.composition import Composition\n",
        "\n",
        "def get_compostion(c):\n",
        "    try:\n",
        "        composition = Composition(c)\n",
        "        return composition\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing formula '{c}': {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "tG4BXyzXb4SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['composition'] = data['Formula'].apply(get_compostion)\n",
        "data['temperature(K)'] = pd.to_numeric(data['temperature(K)'], errors='coerce')\n",
        "data[property_interest] = pd.to_numeric(data[property_interest], errors='coerce')\n"
      ],
      "metadata": {
        "id": "WME5-YVqYNam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "vz1W0Nn1kSLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f =  MultipleFeaturizer([cf.Stoichiometry(), cf.ElementProperty.from_preset(\"magpie\"), cf.ValenceOrbital(props=['avg']), cf.ElementFraction()]) # Featurizers\n",
        "\n",
        "X = np.array(f.featurize_many(data['composition'], ignore_errors=True))\n",
        "measuring_temp_array = np.array(data['temperature(K)']).reshape(-1,1)\n",
        "X = np.hstack((X,measuring_temp_array))\n",
        "\n",
        "y = data[property_interest].values\n",
        "x_df = pd.DataFrame(X)\n",
        "x_df = x_df.loc[:, x_df.std() != 0]\n",
        "print(x_df.shape)"
      ],
      "metadata": {
        "id": "QNrBc9BHl5Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix\n",
        "corr_matrix = x_df_no_constant.corr(method=\"pearson\").abs()\n",
        "\n",
        "# Preview the first 5 rows/columns of the correlation matrix\n",
        "print(corr_matrix.iloc[:5, :5])\n"
      ],
      "metadata": {
        "id": "CKqTsRTyHa4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the features with correlation coefficients above 0.95\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "x_df_low_corr = x_df_no_constant.drop(columns=to_drop)\n",
        "\n",
        "# Recalculate the correlation matrix for the updated features\n",
        "corr_matrix_low_corr = x_df_low_corr.corr(method=\"pearson\").abs()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B2YPz16DJal5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = x_df_low_corr\n",
        "measuring_temp_array = np.array(data['temperature(K)']).reshape(-1,1)"
      ],
      "metadata": {
        "id": "1uX6zKPZlR8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y = data[property_interest].values\n",
        "x_df = pd.DataFrame(X)\n",
        "x_df = x_df.loc[:, x_df.std() != 0]\n",
        "print(x_df.shape)"
      ],
      "metadata": {
        "id": "BkKB9Pc9jE3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6dfb5d-6299-4b90-ee2d-5059b0dbb61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5271, 170)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate standard deviation for each feature in the training data\n",
        "std_dev = np.std(x_df, axis=0)\n",
        "\n",
        "# Identify features with zero standard deviation\n",
        "zero_std_dev_features = np.where(std_dev == 0)[0]\n",
        "# Exclude features with zero standard deviation from both training and test data\n",
        "filtered_x_df = np.delete(x_df, zero_std_dev_features, axis=1)\n",
        "print(filtered_x_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrSkNCrAR9og",
        "outputId": "2d159036-c5d8-467a-ffd6-ec96eef201c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5271, 170)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "all_values = [list(x_df.iloc[x]) for x in range(len(x_df.index))]\n",
        "all_values = np.array(all_values, dtype=float)\n",
        "\n",
        "all_labels = y.copy()\n",
        "\n",
        "train_percent = 0.90\n",
        "index_split_at = int(train_percent * len(all_labels))\n",
        "\n",
        "all_values, all_labels = shuffle(all_values, all_labels, random_state=1)\n",
        "\n",
        "train_values, test_values = np.split(all_values, [index_split_at])\n",
        "train_labels, test_labels = np.split(all_labels, [index_split_at])\n",
        "\n",
        "feature_mean = np.mean(train_values, axis=0)\n",
        "feature_std = np.std(train_values, axis=0)\n",
        "\n",
        "# Check for zero standard deviation before scaling\n",
        "zero_std_dev_features = np.where(feature_std == 0)[0]\n",
        "non_zero_std_dev_features = np.where(feature_std != 0)[0]\n",
        "\n",
        "# Exclude features with zero standard deviation from scaling\n",
        "train_values[:, non_zero_std_dev_features] = (train_values[:, non_zero_std_dev_features] - feature_mean[non_zero_std_dev_features]) / feature_std[non_zero_std_dev_features]\n",
        "test_values[:, non_zero_std_dev_features] = (test_values[:, non_zero_std_dev_features] - feature_mean[non_zero_std_dev_features]) / feature_std[non_zero_std_dev_features]\n",
        "\n",
        "print(\"Shape of Train Values:\", train_values.shape)\n",
        "print(\"Shape of Train Labels:\", train_labels.shape)\n",
        "\n",
        "print(\"Shape of Test Values:\", test_values.shape)\n",
        "print(\"Shape of Test Labels:\", test_labels.shape)\n"
      ],
      "metadata": {
        "id": "3UuN_3RAvgzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#neural network model\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "kernel_init = initializers.RandomNormal(seed=30)\n",
        "bias_init = initializers.Zeros()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(120, activation='relu', use_bias=True, input_shape=(train_values.shape[1],), kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
        "#model.add(Dense(120, activation='relu', use_bias=True, kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
        "model.add(Dense(90, activation='relu', use_bias=True, kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
        "model.add(Dense(60, activation='relu', use_bias=True, kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
        "model.add(Dense(1, activation='linear', use_bias=True, kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
        "\n",
        "optimizer = Adam()\n",
        "\n",
        "model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "# EARLY STOPPING CRITERIAS\n",
        "valmae_es = tf.keras.callbacks.EarlyStopping(monitor='val_mae', min_delta=1e-10, patience=1000, verbose=1, mode='auto', restore_best_weights=True)\n",
        "\n",
        "# EPOCH REAL TIME COUNTER CLASS\n",
        "class PrintEpNum(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        sys.stdout.flush()\n",
        "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" % logs.get('loss') + '                                       \\r')\n",
        "\n",
        "EPOCHS = 30000  # Number of EPOCHS\n",
        "\n",
        "# HISTORY Object which contains how the model learned\n",
        "history = model.fit(train_values, train_labels, batch_size=90, validation_split=0.1, shuffle=False, epochs=EPOCHS, verbose=False, callbacks=[PrintEpNum(), valmae_es])\n",
        "\n",
        "# PLOTTING HISTORY USING MATPLOTLIB\n",
        "plt.figure()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Abs Error')\n",
        "plt.plot(history.epoch, np.array(history.history['mae']), label='MAE Training')\n",
        "plt.plot(history.epoch, np.array(history.history['val_mae']), label='MAE Validation')\n",
        "print(\"Loss at best epoch\", min(list(np.array(history.history['mae']))))\n",
        "plt.legend()\n",
        "plt.savefig('training.pdf')\n",
        "plt.show()\n",
        "\n",
        "[loss, mae] = model.evaluate(train_values, train_labels, verbose=0)\n",
        "print(\"Training Set Mean Absolute Error: {:2.4f} units\".format(mae))\n",
        "\n",
        "[loss, mae] = model.evaluate(test_values, test_labels, verbose=0)\n",
        "print(\"Testing Set Mean Absolute Error: {:2.4f} units\".format(mae))"
      ],
      "metadata": {
        "id": "fof51zJdzy6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#randomforeset model\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Predictions on the test set\n",
        "test_predictions = model.predict(test_values)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE) on the test set\n",
        "mae_test = mean_absolute_error(test_labels, test_predictions)\n",
        "print(\"Test Set Mean Absolute Error: {:2.4f} units\".format(mae_test))\n",
        "\n",
        "# Calculate R-squared (R^2) on the test set\n",
        "r2_test = r2_score(test_labels, test_predictions)\n",
        "print(\"Test Set R-squared (R^2): {:2.4f}\".format(r2_test))"
      ],
      "metadata": {
        "id": "1RouRcT8z8ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create and train the RandomForestRegressor\n",
        "model = RandomForestRegressor(n_estimators=500, random_state=0)\n",
        "model.fit(train_values, train_labels)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_values)\n",
        "\n",
        "# Calculate MAE and R-squared for the test set\n",
        "mae = mean_absolute_error(test_labels, test_predictions)\n",
        "r2 = r2_score(test_labels, test_predictions)\n",
        "\n",
        "print(\"Mean Absolute Error on Test Set:\", mae)\n",
        "print(\"R-squared on Test Set:\", r2)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot actual vs. predicted values\n",
        "plt.scatter(test_labels, test_predictions, color='blue', label='Predicted Values')\n",
        "\n",
        "# Plot a 45-degree line for reference\n",
        "plt.plot([min(test_labels), max(test_labels)], [min(test_labels), max(test_labels)], '--', color='red', label='45-Degree Line')\n",
        "\n",
        "plt.title('Actual vs. Predicted Values on Test Set')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OJbWCmK8wdWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#xgboost model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Shuffle and split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_df, y, test_size=0.1, random_state=40)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'min_child_weight': [1, 2, 3]\n",
        "}\n",
        "\n",
        "# Create an XGBoost regressor\n",
        "xgb_model = XGBRegressor(random_state=40)\n",
        "\n",
        "# Perform GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Create a new XGBoost regressor with the best hyperparameters\n",
        "best_xgb_model = XGBRegressor(**best_params, random_state=40)\n",
        "\n",
        "# Train the model\n",
        "best_xgb_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "xgb_predictions = best_xgb_model.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, xgb_predictions)\n",
        "r2 = r2_score(y_test, xgb_predictions)\n",
        "\n",
        "print(f\"XGBoost MAE: {mae}\")\n",
        "print(f\"XGBoost R-squared: {r2}\")\n"
      ],
      "metadata": {
        "id": "Ff9ohETI5pmn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
